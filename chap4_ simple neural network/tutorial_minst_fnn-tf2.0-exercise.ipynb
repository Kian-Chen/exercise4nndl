{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:24:40.204104: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-23 18:24:40.226502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742725480.251555  827660 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742725480.258814  827660 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-23 18:24:40.285284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742725484.146067  827660 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 74042 MB memory:  -> device: 0, name: NVIDIA A800 80GB PCIe, pci bus id: 0000:4f:00.0, compute capability: 8.0\n",
      "I0000 00:00:1742725484.147779  827660 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79069 MB memory:  -> device: 1, name: NVIDIA A800 80GB PCIe, pci bus id: 0000:52:00.0, compute capability: 8.0\n",
      "I0000 00:00:1742725484.149345  827660 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79069 MB memory:  -> device: 2, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:56:00.0, compute capability: 8.0\n",
      "I0000 00:00:1742725484.150698  827660 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 79069 MB memory:  -> device: 3, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:57:00.0, compute capability: 8.0\n",
      "I0000 00:00:1742725484.152285  827660 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 77485 MB memory:  -> device: 4, name: NVIDIA A800 80GB PCIe, pci bus id: 0000:d5:00.0, compute capability: 8.0\n",
      "I0000 00:00:1742725484.153702  827660 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 128 MB memory:  -> device: 5, name: NVIDIA A800 80GB PCIe, pci bus id: 0000:d6:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################\n",
    "        initializer = tf.keras.initializers.GlorotNormal()\n",
    "        self.W1 = tf.Variable(initializer([28*28, 100]), dtype=tf.float32)\n",
    "        self.b1 = tf.Variable(tf.zeros([100]), dtype=tf.float32)\n",
    "        self.W2 = tf.Variable(initializer([100, 10]), dtype=tf.float32)\n",
    "        self.b2 = tf.Variable(tf.zeros([10]), dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        \n",
    "        h1 = tf.matmul(x, self.W1) + self.b1\n",
    "        h1_relu = tf.nn.relu(h1)\n",
    "        \n",
    "        logits = tf.matmul(h1_relu, self.W2) + self.b2\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 1.2934412 ; accuracy 0.72705\n",
      "epoch 1 : loss 1.2891632 ; accuracy 0.7281167\n",
      "epoch 2 : loss 1.2849207 ; accuracy 0.72915\n",
      "epoch 3 : loss 1.2806945 ; accuracy 0.72996664\n",
      "epoch 4 : loss 1.2764939 ; accuracy 0.7309167\n",
      "epoch 5 : loss 1.2723216 ; accuracy 0.73183334\n",
      "epoch 6 : loss 1.2681676 ; accuracy 0.73286664\n",
      "epoch 7 : loss 1.2640524 ; accuracy 0.73368335\n",
      "epoch 8 : loss 1.2599543 ; accuracy 0.73475\n",
      "epoch 9 : loss 1.2558646 ; accuracy 0.73585\n",
      "epoch 10 : loss 1.2518164 ; accuracy 0.7367167\n",
      "epoch 11 : loss 1.2477885 ; accuracy 0.7377833\n",
      "epoch 12 : loss 1.2437929 ; accuracy 0.73878336\n",
      "epoch 13 : loss 1.2398264 ; accuracy 0.7399\n",
      "epoch 14 : loss 1.235879 ; accuracy 0.74086666\n",
      "epoch 15 : loss 1.2319466 ; accuracy 0.7417167\n",
      "epoch 16 : loss 1.2280463 ; accuracy 0.7428\n",
      "epoch 17 : loss 1.2241628 ; accuracy 0.74371666\n",
      "epoch 18 : loss 1.2203064 ; accuracy 0.7445\n",
      "epoch 19 : loss 1.2164738 ; accuracy 0.74518335\n",
      "epoch 20 : loss 1.212673 ; accuracy 0.74598336\n",
      "epoch 21 : loss 1.2088629 ; accuracy 0.7468\n",
      "epoch 22 : loss 1.2051145 ; accuracy 0.74773335\n",
      "epoch 23 : loss 1.2013756 ; accuracy 0.7486333\n",
      "epoch 24 : loss 1.1976585 ; accuracy 0.7496333\n",
      "epoch 25 : loss 1.1939745 ; accuracy 0.7503667\n",
      "epoch 26 : loss 1.1902982 ; accuracy 0.7510167\n",
      "epoch 27 : loss 1.1866589 ; accuracy 0.7518333\n",
      "epoch 28 : loss 1.1830333 ; accuracy 0.75255\n",
      "epoch 29 : loss 1.1794319 ; accuracy 0.7535167\n",
      "epoch 30 : loss 1.1758544 ; accuracy 0.7543333\n",
      "epoch 31 : loss 1.1722922 ; accuracy 0.7550167\n",
      "epoch 32 : loss 1.1687604 ; accuracy 0.7557167\n",
      "epoch 33 : loss 1.1652583 ; accuracy 0.7566\n",
      "epoch 34 : loss 1.1617622 ; accuracy 0.75733334\n",
      "epoch 35 : loss 1.1583015 ; accuracy 0.7581\n",
      "epoch 36 : loss 1.1548432 ; accuracy 0.75878334\n",
      "epoch 37 : loss 1.1514317 ; accuracy 0.75951666\n",
      "epoch 38 : loss 1.1480235 ; accuracy 0.76016665\n",
      "epoch 39 : loss 1.1446363 ; accuracy 0.76101667\n",
      "epoch 40 : loss 1.1412784 ; accuracy 0.76155\n",
      "epoch 41 : loss 1.1379433 ; accuracy 0.76225\n",
      "epoch 42 : loss 1.1346264 ; accuracy 0.7628667\n",
      "epoch 43 : loss 1.131332 ; accuracy 0.76336664\n",
      "epoch 44 : loss 1.1280578 ; accuracy 0.764\n",
      "epoch 45 : loss 1.1248063 ; accuracy 0.76456666\n",
      "epoch 46 : loss 1.1215706 ; accuracy 0.7652\n",
      "epoch 47 : loss 1.1183649 ; accuracy 0.76586664\n",
      "epoch 48 : loss 1.115179 ; accuracy 0.76655\n",
      "epoch 49 : loss 1.1119974 ; accuracy 0.76738334\n",
      "epoch 50 : loss 1.1088501 ; accuracy 0.7679667\n",
      "epoch 51 : loss 1.1057186 ; accuracy 0.76876664\n",
      "epoch 52 : loss 1.1026069 ; accuracy 0.76956666\n",
      "epoch 53 : loss 1.0995053 ; accuracy 0.77023333\n",
      "epoch 54 : loss 1.0964383 ; accuracy 0.7707833\n",
      "epoch 55 : loss 1.0933924 ; accuracy 0.77126664\n",
      "epoch 56 : loss 1.0903516 ; accuracy 0.77195\n",
      "epoch 57 : loss 1.0873505 ; accuracy 0.7724\n",
      "epoch 58 : loss 1.0843539 ; accuracy 0.7731\n",
      "epoch 59 : loss 1.081379 ; accuracy 0.77381665\n",
      "epoch 60 : loss 1.0784255 ; accuracy 0.7744167\n",
      "epoch 61 : loss 1.0754842 ; accuracy 0.775\n",
      "epoch 62 : loss 1.0725763 ; accuracy 0.7755333\n",
      "epoch 63 : loss 1.0696715 ; accuracy 0.77601665\n",
      "epoch 64 : loss 1.0667814 ; accuracy 0.7765833\n",
      "epoch 65 : loss 1.0639181 ; accuracy 0.7770333\n",
      "epoch 66 : loss 1.0610722 ; accuracy 0.77748334\n",
      "epoch 67 : loss 1.058253 ; accuracy 0.7780833\n",
      "epoch 68 : loss 1.0554417 ; accuracy 0.7785\n",
      "epoch 69 : loss 1.0526611 ; accuracy 0.77896667\n",
      "epoch 70 : loss 1.0498706 ; accuracy 0.7794167\n",
      "epoch 71 : loss 1.0471253 ; accuracy 0.78001666\n",
      "epoch 72 : loss 1.0443698 ; accuracy 0.78055\n",
      "epoch 73 : loss 1.0416535 ; accuracy 0.78111666\n",
      "epoch 74 : loss 1.038957 ; accuracy 0.78175\n",
      "epoch 75 : loss 1.0362645 ; accuracy 0.78215\n",
      "epoch 76 : loss 1.0335975 ; accuracy 0.78256667\n",
      "epoch 77 : loss 1.0309554 ; accuracy 0.78291667\n",
      "epoch 78 : loss 1.0283159 ; accuracy 0.7834333\n",
      "epoch 79 : loss 1.0256867 ; accuracy 0.7837833\n",
      "epoch 80 : loss 1.023079 ; accuracy 0.7843\n",
      "epoch 81 : loss 1.0205077 ; accuracy 0.78473336\n",
      "epoch 82 : loss 1.0179274 ; accuracy 0.785\n",
      "epoch 83 : loss 1.0153795 ; accuracy 0.78536665\n",
      "epoch 84 : loss 1.0128415 ; accuracy 0.78581667\n",
      "epoch 85 : loss 1.0103247 ; accuracy 0.7862\n",
      "epoch 86 : loss 1.0078065 ; accuracy 0.7866167\n",
      "epoch 87 : loss 1.0053213 ; accuracy 0.78693336\n",
      "epoch 88 : loss 1.0028392 ; accuracy 0.7873333\n",
      "epoch 89 : loss 1.0003815 ; accuracy 0.7877167\n",
      "epoch 90 : loss 0.99794894 ; accuracy 0.78818333\n",
      "epoch 91 : loss 0.99550474 ; accuracy 0.78866667\n",
      "epoch 92 : loss 0.99308866 ; accuracy 0.78893334\n",
      "epoch 93 : loss 0.99069303 ; accuracy 0.78935\n",
      "epoch 94 : loss 0.98831445 ; accuracy 0.78971666\n",
      "epoch 95 : loss 0.9859383 ; accuracy 0.7902667\n",
      "epoch 96 : loss 0.9835791 ; accuracy 0.79073334\n",
      "epoch 97 : loss 0.981234 ; accuracy 0.79141665\n",
      "epoch 98 : loss 0.9789031 ; accuracy 0.7920167\n",
      "epoch 99 : loss 0.9765934 ; accuracy 0.7924167\n",
      "epoch 100 : loss 0.97428787 ; accuracy 0.79275\n",
      "epoch 101 : loss 0.9720107 ; accuracy 0.79315\n",
      "epoch 102 : loss 0.9697272 ; accuracy 0.7935\n",
      "epoch 103 : loss 0.9674848 ; accuracy 0.79373336\n",
      "epoch 104 : loss 0.9652268 ; accuracy 0.7941\n",
      "epoch 105 : loss 0.9629929 ; accuracy 0.79443336\n",
      "epoch 106 : loss 0.9607845 ; accuracy 0.79478335\n",
      "epoch 107 : loss 0.95858115 ; accuracy 0.7951\n",
      "epoch 108 : loss 0.95639336 ; accuracy 0.7956\n",
      "epoch 109 : loss 0.9542085 ; accuracy 0.79595\n",
      "epoch 110 : loss 0.95204407 ; accuracy 0.7964\n",
      "epoch 111 : loss 0.9498967 ; accuracy 0.7967\n",
      "epoch 112 : loss 0.9477452 ; accuracy 0.79711664\n",
      "epoch 113 : loss 0.94563 ; accuracy 0.79751664\n",
      "epoch 114 : loss 0.94351846 ; accuracy 0.79786664\n",
      "epoch 115 : loss 0.94141555 ; accuracy 0.79826665\n",
      "epoch 116 : loss 0.9393237 ; accuracy 0.79861665\n",
      "epoch 117 : loss 0.93724555 ; accuracy 0.79903334\n",
      "epoch 118 : loss 0.93519026 ; accuracy 0.79925\n",
      "epoch 119 : loss 0.93313396 ; accuracy 0.7996\n",
      "epoch 120 : loss 0.9310988 ; accuracy 0.8\n",
      "epoch 121 : loss 0.9290763 ; accuracy 0.8002833\n",
      "epoch 122 : loss 0.9270476 ; accuracy 0.80058336\n",
      "epoch 123 : loss 0.9250471 ; accuracy 0.80086666\n",
      "epoch 124 : loss 0.9230482 ; accuracy 0.80118334\n",
      "epoch 125 : loss 0.9210586 ; accuracy 0.8015\n",
      "epoch 126 : loss 0.9190966 ; accuracy 0.80183333\n",
      "epoch 127 : loss 0.917139 ; accuracy 0.8023667\n",
      "epoch 128 : loss 0.91519046 ; accuracy 0.8027167\n",
      "epoch 129 : loss 0.9132552 ; accuracy 0.80298334\n",
      "epoch 130 : loss 0.91133094 ; accuracy 0.80315\n",
      "epoch 131 : loss 0.9094202 ; accuracy 0.8035667\n",
      "epoch 132 : loss 0.9075098 ; accuracy 0.804\n",
      "epoch 133 : loss 0.9056244 ; accuracy 0.80441666\n",
      "epoch 134 : loss 0.9037332 ; accuracy 0.80471665\n",
      "epoch 135 : loss 0.9018725 ; accuracy 0.8050167\n",
      "epoch 136 : loss 0.8999974 ; accuracy 0.80543333\n",
      "epoch 137 : loss 0.8981526 ; accuracy 0.80576664\n",
      "epoch 138 : loss 0.8963163 ; accuracy 0.8060833\n",
      "epoch 139 : loss 0.8944922 ; accuracy 0.80635\n",
      "epoch 140 : loss 0.892668 ; accuracy 0.8067167\n",
      "epoch 141 : loss 0.8908608 ; accuracy 0.8070667\n",
      "epoch 142 : loss 0.8890564 ; accuracy 0.80735\n",
      "epoch 143 : loss 0.8872722 ; accuracy 0.8077833\n",
      "epoch 144 : loss 0.88548696 ; accuracy 0.80815\n",
      "epoch 145 : loss 0.88371456 ; accuracy 0.8084667\n",
      "epoch 146 : loss 0.8819526 ; accuracy 0.80865\n",
      "epoch 147 : loss 0.88020694 ; accuracy 0.80901664\n",
      "epoch 148 : loss 0.87846845 ; accuracy 0.8092167\n",
      "epoch 149 : loss 0.87673366 ; accuracy 0.80943334\n",
      "epoch 150 : loss 0.87501717 ; accuracy 0.8095833\n",
      "epoch 151 : loss 0.8733042 ; accuracy 0.80986667\n",
      "epoch 152 : loss 0.8716015 ; accuracy 0.81016666\n",
      "epoch 153 : loss 0.86991006 ; accuracy 0.81053334\n",
      "epoch 154 : loss 0.86822504 ; accuracy 0.81085\n",
      "epoch 155 : loss 0.8665525 ; accuracy 0.8112\n",
      "epoch 156 : loss 0.8648806 ; accuracy 0.81135\n",
      "epoch 157 : loss 0.86323196 ; accuracy 0.81161666\n",
      "epoch 158 : loss 0.8615781 ; accuracy 0.81191665\n",
      "epoch 159 : loss 0.8599509 ; accuracy 0.8121333\n",
      "epoch 160 : loss 0.85831606 ; accuracy 0.81243336\n",
      "epoch 161 : loss 0.85669196 ; accuracy 0.81271666\n",
      "epoch 162 : loss 0.8550773 ; accuracy 0.81296664\n",
      "epoch 163 : loss 0.85347795 ; accuracy 0.81335\n",
      "epoch 164 : loss 0.8518829 ; accuracy 0.8135667\n",
      "epoch 165 : loss 0.8502937 ; accuracy 0.81385\n",
      "epoch 166 : loss 0.84872186 ; accuracy 0.8142667\n",
      "epoch 167 : loss 0.84715116 ; accuracy 0.81458336\n",
      "epoch 168 : loss 0.8455887 ; accuracy 0.81475\n",
      "epoch 169 : loss 0.8440327 ; accuracy 0.8149\n",
      "epoch 170 : loss 0.8424861 ; accuracy 0.81521666\n",
      "epoch 171 : loss 0.84095454 ; accuracy 0.8154\n",
      "epoch 172 : loss 0.83941555 ; accuracy 0.81563336\n",
      "epoch 173 : loss 0.8379036 ; accuracy 0.81595\n",
      "epoch 174 : loss 0.8363915 ; accuracy 0.81625\n",
      "epoch 175 : loss 0.83488345 ; accuracy 0.81655\n",
      "epoch 176 : loss 0.83339256 ; accuracy 0.8168833\n",
      "epoch 177 : loss 0.8318975 ; accuracy 0.8171333\n",
      "epoch 178 : loss 0.8304107 ; accuracy 0.81731665\n",
      "epoch 179 : loss 0.8289423 ; accuracy 0.81745\n",
      "epoch 180 : loss 0.8274714 ; accuracy 0.81766665\n",
      "epoch 181 : loss 0.82601464 ; accuracy 0.8178833\n",
      "epoch 182 : loss 0.824563 ; accuracy 0.81811666\n",
      "epoch 183 : loss 0.8231099 ; accuracy 0.8184\n",
      "epoch 184 : loss 0.82167953 ; accuracy 0.8185167\n",
      "epoch 185 : loss 0.8202572 ; accuracy 0.8185833\n",
      "epoch 186 : loss 0.8188186 ; accuracy 0.81881666\n",
      "epoch 187 : loss 0.8174154 ; accuracy 0.819\n",
      "epoch 188 : loss 0.8160029 ; accuracy 0.81913334\n",
      "epoch 189 : loss 0.81460834 ; accuracy 0.8193\n",
      "epoch 190 : loss 0.8132087 ; accuracy 0.81945\n",
      "epoch 191 : loss 0.8118263 ; accuracy 0.8197\n",
      "epoch 192 : loss 0.8104358 ; accuracy 0.81983334\n",
      "epoch 193 : loss 0.8090705 ; accuracy 0.82011664\n",
      "epoch 194 : loss 0.80770206 ; accuracy 0.8202\n",
      "epoch 195 : loss 0.8063479 ; accuracy 0.82055\n",
      "epoch 196 : loss 0.80499446 ; accuracy 0.82061666\n",
      "epoch 197 : loss 0.80364525 ; accuracy 0.8208333\n",
      "epoch 198 : loss 0.802304 ; accuracy 0.821\n",
      "epoch 199 : loss 0.8009723 ; accuracy 0.8211667\n",
      "epoch 200 : loss 0.79964924 ; accuracy 0.82145\n",
      "epoch 201 : loss 0.7983249 ; accuracy 0.82173336\n",
      "epoch 202 : loss 0.79701066 ; accuracy 0.82195\n",
      "epoch 203 : loss 0.7957135 ; accuracy 0.8221833\n",
      "epoch 204 : loss 0.7944009 ; accuracy 0.82248336\n",
      "epoch 205 : loss 0.7931177 ; accuracy 0.8225667\n",
      "epoch 206 : loss 0.79182005 ; accuracy 0.82278335\n",
      "epoch 207 : loss 0.7905483 ; accuracy 0.823\n",
      "epoch 208 : loss 0.78926194 ; accuracy 0.82316667\n",
      "epoch 209 : loss 0.7880043 ; accuracy 0.8233333\n",
      "epoch 210 : loss 0.78672904 ; accuracy 0.82343334\n",
      "epoch 211 : loss 0.78547883 ; accuracy 0.8237333\n",
      "epoch 212 : loss 0.78422475 ; accuracy 0.8240333\n",
      "epoch 213 : loss 0.7829835 ; accuracy 0.82423335\n",
      "epoch 214 : loss 0.7817504 ; accuracy 0.8243833\n",
      "epoch 215 : loss 0.780512 ; accuracy 0.82445\n",
      "epoch 216 : loss 0.77928704 ; accuracy 0.8246667\n",
      "epoch 217 : loss 0.77806115 ; accuracy 0.8247833\n",
      "epoch 218 : loss 0.77683735 ; accuracy 0.82491666\n",
      "epoch 219 : loss 0.7756358 ; accuracy 0.8251\n",
      "epoch 220 : loss 0.77443093 ; accuracy 0.82528335\n",
      "epoch 221 : loss 0.77322996 ; accuracy 0.82551664\n",
      "epoch 222 : loss 0.7720356 ; accuracy 0.8257167\n",
      "epoch 223 : loss 0.7708453 ; accuracy 0.8258833\n",
      "epoch 224 : loss 0.7696634 ; accuracy 0.8262\n",
      "epoch 225 : loss 0.7684918 ; accuracy 0.82628334\n",
      "epoch 226 : loss 0.7673125 ; accuracy 0.8264833\n",
      "epoch 227 : loss 0.76615286 ; accuracy 0.82655\n",
      "epoch 228 : loss 0.7649908 ; accuracy 0.82668334\n",
      "epoch 229 : loss 0.7638394 ; accuracy 0.8268833\n",
      "epoch 230 : loss 0.76268935 ; accuracy 0.82715\n",
      "epoch 231 : loss 0.7615391 ; accuracy 0.82735\n",
      "epoch 232 : loss 0.76040125 ; accuracy 0.8275333\n",
      "epoch 233 : loss 0.7592706 ; accuracy 0.8277\n",
      "epoch 234 : loss 0.7581394 ; accuracy 0.82795\n",
      "epoch 235 : loss 0.7570161 ; accuracy 0.8282167\n",
      "epoch 236 : loss 0.75589633 ; accuracy 0.8283833\n",
      "epoch 237 : loss 0.7547751 ; accuracy 0.8286\n",
      "epoch 238 : loss 0.75366956 ; accuracy 0.8287167\n",
      "epoch 239 : loss 0.75256646 ; accuracy 0.82883334\n",
      "epoch 240 : loss 0.7514681 ; accuracy 0.8291\n",
      "epoch 241 : loss 0.7503769 ; accuracy 0.82925\n",
      "epoch 242 : loss 0.7492836 ; accuracy 0.8293167\n",
      "epoch 243 : loss 0.74820566 ; accuracy 0.82948333\n",
      "epoch 244 : loss 0.74712133 ; accuracy 0.8297167\n",
      "epoch 245 : loss 0.7460524 ; accuracy 0.82985\n",
      "epoch 246 : loss 0.7449813 ; accuracy 0.82995\n",
      "epoch 247 : loss 0.7439165 ; accuracy 0.8301333\n",
      "epoch 248 : loss 0.74285626 ; accuracy 0.83023334\n",
      "epoch 249 : loss 0.74180406 ; accuracy 0.83038336\n",
      "epoch 250 : loss 0.7407513 ; accuracy 0.8305333\n",
      "epoch 251 : loss 0.7397019 ; accuracy 0.83073336\n",
      "epoch 252 : loss 0.73866445 ; accuracy 0.8308167\n",
      "epoch 253 : loss 0.7376193 ; accuracy 0.83096665\n",
      "epoch 254 : loss 0.73659456 ; accuracy 0.8311\n",
      "epoch 255 : loss 0.7355596 ; accuracy 0.8312333\n",
      "epoch 256 : loss 0.73453945 ; accuracy 0.83141667\n",
      "epoch 257 : loss 0.7335239 ; accuracy 0.83161664\n",
      "epoch 258 : loss 0.7325067 ; accuracy 0.83183336\n",
      "epoch 259 : loss 0.7314952 ; accuracy 0.83196664\n",
      "epoch 260 : loss 0.7304927 ; accuracy 0.8321\n",
      "epoch 261 : loss 0.7294875 ; accuracy 0.83235\n",
      "epoch 262 : loss 0.72849065 ; accuracy 0.8325167\n",
      "epoch 263 : loss 0.7275042 ; accuracy 0.8326167\n",
      "epoch 264 : loss 0.72651035 ; accuracy 0.83275\n",
      "epoch 265 : loss 0.7255289 ; accuracy 0.8329167\n",
      "epoch 266 : loss 0.7245523 ; accuracy 0.83315\n",
      "epoch 267 : loss 0.7235738 ; accuracy 0.83318335\n",
      "epoch 268 : loss 0.7226082 ; accuracy 0.8333333\n",
      "epoch 269 : loss 0.72163284 ; accuracy 0.83345\n",
      "epoch 270 : loss 0.72067344 ; accuracy 0.83356667\n",
      "epoch 271 : loss 0.7197164 ; accuracy 0.8337333\n",
      "epoch 272 : loss 0.71875805 ; accuracy 0.83393335\n",
      "epoch 273 : loss 0.717806 ; accuracy 0.834\n",
      "epoch 274 : loss 0.7168594 ; accuracy 0.8341333\n",
      "epoch 275 : loss 0.71591353 ; accuracy 0.8343833\n",
      "epoch 276 : loss 0.71497893 ; accuracy 0.8344667\n",
      "epoch 277 : loss 0.7140385 ; accuracy 0.8347\n",
      "epoch 278 : loss 0.7131121 ; accuracy 0.83496666\n",
      "epoch 279 : loss 0.7121891 ; accuracy 0.8351833\n",
      "epoch 280 : loss 0.7112566 ; accuracy 0.83536667\n",
      "epoch 281 : loss 0.71033907 ; accuracy 0.8355333\n",
      "epoch 282 : loss 0.7094269 ; accuracy 0.83563334\n",
      "epoch 283 : loss 0.70851344 ; accuracy 0.8358333\n",
      "epoch 284 : loss 0.70761156 ; accuracy 0.83596665\n",
      "epoch 285 : loss 0.7066975 ; accuracy 0.8362333\n",
      "epoch 286 : loss 0.705805 ; accuracy 0.83631665\n",
      "epoch 287 : loss 0.70490545 ; accuracy 0.83641666\n",
      "epoch 288 : loss 0.70401 ; accuracy 0.83666664\n",
      "epoch 289 : loss 0.7031226 ; accuracy 0.8368833\n",
      "epoch 290 : loss 0.7022332 ; accuracy 0.8369667\n",
      "epoch 291 : loss 0.7013561 ; accuracy 0.83708334\n",
      "epoch 292 : loss 0.7004732 ; accuracy 0.8372333\n",
      "epoch 293 : loss 0.6996013 ; accuracy 0.83746666\n",
      "epoch 294 : loss 0.6987283 ; accuracy 0.8376167\n",
      "epoch 295 : loss 0.6978634 ; accuracy 0.83775\n",
      "epoch 296 : loss 0.69700015 ; accuracy 0.83786666\n",
      "epoch 297 : loss 0.6961381 ; accuracy 0.838\n",
      "epoch 298 : loss 0.6952814 ; accuracy 0.83821666\n",
      "epoch 299 : loss 0.6944276 ; accuracy 0.8383833\n",
      "epoch 300 : loss 0.6935758 ; accuracy 0.83855\n",
      "epoch 301 : loss 0.69272923 ; accuracy 0.8387167\n",
      "epoch 302 : loss 0.69188404 ; accuracy 0.83881664\n",
      "epoch 303 : loss 0.6910482 ; accuracy 0.83893335\n",
      "epoch 304 : loss 0.6902095 ; accuracy 0.8391333\n",
      "epoch 305 : loss 0.6893747 ; accuracy 0.83931667\n",
      "epoch 306 : loss 0.6885441 ; accuracy 0.8394333\n",
      "epoch 307 : loss 0.6877207 ; accuracy 0.83965\n",
      "epoch 308 : loss 0.68688864 ; accuracy 0.83975\n",
      "epoch 309 : loss 0.6860688 ; accuracy 0.8399\n",
      "epoch 310 : loss 0.68525875 ; accuracy 0.83996665\n",
      "epoch 311 : loss 0.68444175 ; accuracy 0.84008336\n",
      "epoch 312 : loss 0.68362784 ; accuracy 0.8401833\n",
      "epoch 313 : loss 0.68282175 ; accuracy 0.84028333\n",
      "epoch 314 : loss 0.682018 ; accuracy 0.8404\n",
      "epoch 315 : loss 0.68121177 ; accuracy 0.8405167\n",
      "epoch 316 : loss 0.6804189 ; accuracy 0.84068334\n",
      "epoch 317 : loss 0.6796248 ; accuracy 0.84071666\n",
      "epoch 318 : loss 0.6788301 ; accuracy 0.84085\n",
      "epoch 319 : loss 0.6780404 ; accuracy 0.8409333\n",
      "epoch 320 : loss 0.6772565 ; accuracy 0.84105\n",
      "epoch 321 : loss 0.6764723 ; accuracy 0.8411833\n",
      "epoch 322 : loss 0.6756897 ; accuracy 0.84136665\n",
      "epoch 323 : loss 0.6749162 ; accuracy 0.84148335\n",
      "epoch 324 : loss 0.67414284 ; accuracy 0.8416333\n",
      "epoch 325 : loss 0.673366 ; accuracy 0.84171665\n",
      "epoch 326 : loss 0.6726053 ; accuracy 0.8418667\n",
      "epoch 327 : loss 0.6718352 ; accuracy 0.8420333\n",
      "epoch 328 : loss 0.67107177 ; accuracy 0.8421\n",
      "epoch 329 : loss 0.6703137 ; accuracy 0.8423167\n",
      "epoch 330 : loss 0.6695587 ; accuracy 0.84246665\n",
      "epoch 331 : loss 0.6688077 ; accuracy 0.8426167\n",
      "epoch 332 : loss 0.6680557 ; accuracy 0.8428\n",
      "epoch 333 : loss 0.6673047 ; accuracy 0.84288335\n",
      "epoch 334 : loss 0.66656244 ; accuracy 0.8429667\n",
      "epoch 335 : loss 0.6658179 ; accuracy 0.8430333\n",
      "epoch 336 : loss 0.6650737 ; accuracy 0.84311664\n",
      "epoch 337 : loss 0.6643425 ; accuracy 0.84328336\n",
      "epoch 338 : loss 0.66360795 ; accuracy 0.8433333\n",
      "epoch 339 : loss 0.6628751 ; accuracy 0.8434833\n",
      "epoch 340 : loss 0.66215 ; accuracy 0.84353334\n",
      "epoch 341 : loss 0.661421 ; accuracy 0.84361666\n",
      "epoch 342 : loss 0.6606973 ; accuracy 0.8437333\n",
      "epoch 343 : loss 0.6599762 ; accuracy 0.8438333\n",
      "epoch 344 : loss 0.65926224 ; accuracy 0.84403336\n",
      "epoch 345 : loss 0.65854603 ; accuracy 0.8441\n",
      "epoch 346 : loss 0.65783423 ; accuracy 0.84431666\n",
      "epoch 347 : loss 0.6571213 ; accuracy 0.84435\n",
      "epoch 348 : loss 0.6564094 ; accuracy 0.8445\n",
      "epoch 349 : loss 0.6557163 ; accuracy 0.84458333\n",
      "epoch 350 : loss 0.6550104 ; accuracy 0.84475\n",
      "epoch 351 : loss 0.65431017 ; accuracy 0.84485\n",
      "epoch 352 : loss 0.65361595 ; accuracy 0.84495\n",
      "epoch 353 : loss 0.65291744 ; accuracy 0.8451667\n",
      "epoch 354 : loss 0.6522304 ; accuracy 0.8452833\n",
      "epoch 355 : loss 0.65152746 ; accuracy 0.84538335\n",
      "epoch 356 : loss 0.6508521 ; accuracy 0.8455167\n",
      "epoch 357 : loss 0.65016806 ; accuracy 0.84566665\n",
      "epoch 358 : loss 0.64948845 ; accuracy 0.84575\n",
      "epoch 359 : loss 0.6488068 ; accuracy 0.8459167\n",
      "epoch 360 : loss 0.6481311 ; accuracy 0.84601665\n",
      "epoch 361 : loss 0.6474528 ; accuracy 0.8462\n",
      "epoch 362 : loss 0.6467819 ; accuracy 0.8463167\n",
      "epoch 363 : loss 0.6461134 ; accuracy 0.84648335\n",
      "epoch 364 : loss 0.645443 ; accuracy 0.8466833\n",
      "epoch 365 : loss 0.6447785 ; accuracy 0.8469167\n",
      "epoch 366 : loss 0.64411646 ; accuracy 0.84706664\n",
      "epoch 367 : loss 0.6434572 ; accuracy 0.84723336\n",
      "epoch 368 : loss 0.64279974 ; accuracy 0.8473333\n",
      "epoch 369 : loss 0.64213973 ; accuracy 0.84748334\n",
      "epoch 370 : loss 0.6414942 ; accuracy 0.8476167\n",
      "epoch 371 : loss 0.6408367 ; accuracy 0.84776664\n",
      "epoch 372 : loss 0.6401873 ; accuracy 0.84788334\n",
      "epoch 373 : loss 0.6395443 ; accuracy 0.84798336\n",
      "epoch 374 : loss 0.6388974 ; accuracy 0.8481\n",
      "epoch 375 : loss 0.6382583 ; accuracy 0.84821665\n",
      "epoch 376 : loss 0.6376198 ; accuracy 0.84828335\n",
      "epoch 377 : loss 0.63698256 ; accuracy 0.84835\n",
      "epoch 378 : loss 0.63635194 ; accuracy 0.84845\n",
      "epoch 379 : loss 0.6357102 ; accuracy 0.84856665\n",
      "epoch 380 : loss 0.63508016 ; accuracy 0.8487\n",
      "epoch 381 : loss 0.6344581 ; accuracy 0.84875\n",
      "epoch 382 : loss 0.63382316 ; accuracy 0.8488\n",
      "epoch 383 : loss 0.633202 ; accuracy 0.84901667\n",
      "epoch 384 : loss 0.63258123 ; accuracy 0.84915\n",
      "epoch 385 : loss 0.6319651 ; accuracy 0.8491833\n",
      "epoch 386 : loss 0.6313425 ; accuracy 0.8493\n",
      "epoch 387 : loss 0.6307254 ; accuracy 0.8494833\n",
      "epoch 388 : loss 0.630113 ; accuracy 0.8495833\n",
      "epoch 389 : loss 0.6295038 ; accuracy 0.8497\n",
      "epoch 390 : loss 0.6288926 ; accuracy 0.8498\n",
      "epoch 391 : loss 0.62829 ; accuracy 0.8499\n",
      "epoch 392 : loss 0.62767774 ; accuracy 0.84998333\n",
      "epoch 393 : loss 0.62707543 ; accuracy 0.85005\n",
      "epoch 394 : loss 0.6264785 ; accuracy 0.8501\n",
      "epoch 395 : loss 0.62587565 ; accuracy 0.85031664\n",
      "epoch 396 : loss 0.62527776 ; accuracy 0.85041666\n",
      "epoch 397 : loss 0.62468505 ; accuracy 0.85048336\n",
      "epoch 398 : loss 0.62408775 ; accuracy 0.8506\n",
      "epoch 399 : loss 0.6234963 ; accuracy 0.85075\n",
      "epoch 400 : loss 0.6229091 ; accuracy 0.85081667\n",
      "epoch 401 : loss 0.622326 ; accuracy 0.85106665\n",
      "epoch 402 : loss 0.6217401 ; accuracy 0.85116667\n",
      "epoch 403 : loss 0.621154 ; accuracy 0.8512667\n",
      "epoch 404 : loss 0.62057453 ; accuracy 0.8513333\n",
      "epoch 405 : loss 0.6200007 ; accuracy 0.8514\n",
      "epoch 406 : loss 0.61941797 ; accuracy 0.85151666\n",
      "epoch 407 : loss 0.6188386 ; accuracy 0.8516333\n",
      "epoch 408 : loss 0.6182626 ; accuracy 0.85178334\n",
      "epoch 409 : loss 0.6176965 ; accuracy 0.85188335\n",
      "epoch 410 : loss 0.6171249 ; accuracy 0.85205\n",
      "epoch 411 : loss 0.6165585 ; accuracy 0.8521\n",
      "epoch 412 : loss 0.6159922 ; accuracy 0.85225\n",
      "epoch 413 : loss 0.6154291 ; accuracy 0.8523833\n",
      "epoch 414 : loss 0.61486596 ; accuracy 0.85258335\n",
      "epoch 415 : loss 0.6143042 ; accuracy 0.85261667\n",
      "epoch 416 : loss 0.6137454 ; accuracy 0.8528\n",
      "epoch 417 : loss 0.61319274 ; accuracy 0.85291666\n",
      "epoch 418 : loss 0.61263996 ; accuracy 0.8530333\n",
      "epoch 419 : loss 0.6120802 ; accuracy 0.85315\n",
      "epoch 420 : loss 0.6115304 ; accuracy 0.85325\n",
      "epoch 421 : loss 0.6109838 ; accuracy 0.8533\n",
      "epoch 422 : loss 0.61043423 ; accuracy 0.8534\n",
      "epoch 423 : loss 0.6098896 ; accuracy 0.8534833\n",
      "epoch 424 : loss 0.60934335 ; accuracy 0.85358334\n",
      "epoch 425 : loss 0.6087991 ; accuracy 0.85373336\n",
      "epoch 426 : loss 0.60826147 ; accuracy 0.8538\n",
      "epoch 427 : loss 0.6077203 ; accuracy 0.85396665\n",
      "epoch 428 : loss 0.60718435 ; accuracy 0.85403335\n",
      "epoch 429 : loss 0.606647 ; accuracy 0.85406667\n",
      "epoch 430 : loss 0.6061153 ; accuracy 0.85408336\n",
      "epoch 431 : loss 0.60558206 ; accuracy 0.8542\n",
      "epoch 432 : loss 0.6050536 ; accuracy 0.85426664\n",
      "epoch 433 : loss 0.6045191 ; accuracy 0.8544667\n",
      "epoch 434 : loss 0.6039939 ; accuracy 0.8545667\n",
      "epoch 435 : loss 0.60346997 ; accuracy 0.8545667\n",
      "epoch 436 : loss 0.60294557 ; accuracy 0.85463333\n",
      "epoch 437 : loss 0.6024193 ; accuracy 0.85471666\n",
      "epoch 438 : loss 0.6018981 ; accuracy 0.8548\n",
      "epoch 439 : loss 0.60138386 ; accuracy 0.85485\n",
      "epoch 440 : loss 0.6008648 ; accuracy 0.8549167\n",
      "epoch 441 : loss 0.60035163 ; accuracy 0.8549\n",
      "epoch 442 : loss 0.5998349 ; accuracy 0.85503334\n",
      "epoch 443 : loss 0.59932154 ; accuracy 0.85513335\n",
      "epoch 444 : loss 0.59881055 ; accuracy 0.8551667\n",
      "epoch 445 : loss 0.5983012 ; accuracy 0.8552333\n",
      "epoch 446 : loss 0.5977919 ; accuracy 0.85525\n",
      "epoch 447 : loss 0.59728384 ; accuracy 0.85536665\n",
      "epoch 448 : loss 0.5967848 ; accuracy 0.85545\n",
      "epoch 449 : loss 0.5962777 ; accuracy 0.8556\n",
      "epoch 450 : loss 0.5957762 ; accuracy 0.85566664\n",
      "epoch 451 : loss 0.5952767 ; accuracy 0.85571665\n",
      "epoch 452 : loss 0.5947788 ; accuracy 0.85571665\n",
      "epoch 453 : loss 0.59428126 ; accuracy 0.85575\n",
      "epoch 454 : loss 0.5937845 ; accuracy 0.8558\n",
      "epoch 455 : loss 0.59329367 ; accuracy 0.8559167\n",
      "epoch 456 : loss 0.59280014 ; accuracy 0.8559833\n",
      "epoch 457 : loss 0.59230804 ; accuracy 0.85606664\n",
      "epoch 458 : loss 0.59181994 ; accuracy 0.85616666\n",
      "epoch 459 : loss 0.59132797 ; accuracy 0.85625\n",
      "epoch 460 : loss 0.5908445 ; accuracy 0.85635\n",
      "epoch 461 : loss 0.59035957 ; accuracy 0.85641664\n",
      "epoch 462 : loss 0.58987355 ; accuracy 0.85643333\n",
      "epoch 463 : loss 0.5893893 ; accuracy 0.85653335\n",
      "epoch 464 : loss 0.5889095 ; accuracy 0.8566\n",
      "epoch 465 : loss 0.58843213 ; accuracy 0.8566\n",
      "epoch 466 : loss 0.5879529 ; accuracy 0.8567167\n",
      "epoch 467 : loss 0.5874767 ; accuracy 0.85675\n",
      "epoch 468 : loss 0.5870073 ; accuracy 0.85686666\n",
      "epoch 469 : loss 0.58652645 ; accuracy 0.8569667\n",
      "epoch 470 : loss 0.58605283 ; accuracy 0.8570333\n",
      "epoch 471 : loss 0.5855889 ; accuracy 0.85711664\n",
      "epoch 472 : loss 0.58511275 ; accuracy 0.85718334\n",
      "epoch 473 : loss 0.58464795 ; accuracy 0.85723335\n",
      "epoch 474 : loss 0.5841834 ; accuracy 0.85728335\n",
      "epoch 475 : loss 0.583713 ; accuracy 0.8574333\n",
      "epoch 476 : loss 0.58325505 ; accuracy 0.85745\n",
      "epoch 477 : loss 0.5827904 ; accuracy 0.85756665\n",
      "epoch 478 : loss 0.5823298 ; accuracy 0.85763335\n",
      "epoch 479 : loss 0.5818673 ; accuracy 0.8577833\n",
      "epoch 480 : loss 0.5814107 ; accuracy 0.85786664\n",
      "epoch 481 : loss 0.5809537 ; accuracy 0.85795\n",
      "epoch 482 : loss 0.58050066 ; accuracy 0.85796666\n",
      "epoch 483 : loss 0.58004516 ; accuracy 0.85801667\n",
      "epoch 484 : loss 0.57959086 ; accuracy 0.85805\n",
      "epoch 485 : loss 0.579137 ; accuracy 0.85823333\n",
      "epoch 486 : loss 0.57868886 ; accuracy 0.85835\n",
      "epoch 487 : loss 0.5782391 ; accuracy 0.8584167\n",
      "epoch 488 : loss 0.577796 ; accuracy 0.85845\n",
      "epoch 489 : loss 0.5773447 ; accuracy 0.8584667\n",
      "epoch 490 : loss 0.5768991 ; accuracy 0.8585\n",
      "epoch 491 : loss 0.5764577 ; accuracy 0.8584833\n",
      "epoch 492 : loss 0.57601535 ; accuracy 0.85858333\n",
      "epoch 493 : loss 0.5755764 ; accuracy 0.85861665\n",
      "epoch 494 : loss 0.57513267 ; accuracy 0.85873336\n",
      "epoch 495 : loss 0.5746958 ; accuracy 0.8588333\n",
      "epoch 496 : loss 0.5742561 ; accuracy 0.8589333\n",
      "epoch 497 : loss 0.57382363 ; accuracy 0.859\n",
      "epoch 498 : loss 0.5733876 ; accuracy 0.85915\n",
      "epoch 499 : loss 0.57295525 ; accuracy 0.8592333\n",
      "test loss 0.5550467 ; accuracy 0.8654\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(500):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
